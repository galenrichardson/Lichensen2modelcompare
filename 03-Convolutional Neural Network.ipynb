{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3179266a",
   "metadata": {},
   "source": [
    "Citation: Richardson G, Knudby A, Chen W, Sawada M, Lovitt J, He L, et al. (2023) Dense neural network outperforms other machine learning models for scaling-up lichen cover maps in Eastern Canada. PLoS ONE 18(11): e0292839. https://doi.org/10.1371/journal.pone.0292839"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea060016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, datetime, csv,math,scipy,time\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import pearsonr\n",
    "#displaying data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.pyplot import imshow,figure\n",
    "import matplotlib.image as mpimg\n",
    "plt.rcParams['figure.dpi'] = 150 \n",
    "%matplotlib inline\n",
    "#TF imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras import backend as K \n",
    "temp_weights=r'D:\\Training_Data_Creation\\01-logs\\temp.h5'\n",
    "'''TF code to tell TF version, GPU detected, and limit memory growth'''\n",
    "print(f\"Tensorflow ver. {tf.__version__}\")\n",
    "physical_device = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(f'Device found : {physical_device}')\n",
    "tf.config.experimental.get_memory_growth(physical_device[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8670d2",
   "metadata": {},
   "source": [
    "## Variables and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c181e043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb.fs \n",
    "#Importing metric functions\n",
    "from .defs.Thesis_Functions import dtime,calculate_metrics,time_and_metrics\n",
    "#Importing plotting functions\n",
    "from .defs.Thesis_Functions import make_scatter_from_results,plot_hist_save\n",
    "#Importing NN functions\n",
    "from .defs.Thesis_Functions import get_model_mae,save_nn_for_log,nnDset_to_Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921f9dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create training data in TF\n",
    "Train_txt = r'D:\\Training_Data_Creation\\Pointer_files\\MidpixTrain_ts3-mega40.txt'\n",
    "val_txt = r'D:\\Training_Data_Creation\\Pointer_files\\MidpixVal_ts3-mega40.txt'\n",
    "Test_txt = r'D:\\Training_Data_Creation\\Pointer_files\\MidpixTest_ts3-mega40.txt'\n",
    "Testp_txt = r'D:\\Training_Data_Creation\\Pointer_files\\MidpixTestP40_Mega_ts3-mega50.txt'\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "#Lists where to store the paths and labels\n",
    "def txt_to_list(filename):\n",
    "    joint = []\n",
    "    with open(filename, 'r') as File:\n",
    "        infoFile = File.readlines() #Reading all the lines from File\n",
    "        for line in infoFile: #Reading line-by-line\n",
    "            words = line.split() #Splitting lines in words using space character as separator\n",
    "            joint.append((words[0],words[1],words[2],words[3]))\n",
    "    #print ('sample{}\\n{} files'.format(joint[0],str(len(joint))))\n",
    "    return joint\n",
    "#for loading both images and labels\n",
    "def load_img(img):\n",
    "    image = tf.io.read_file(img)\n",
    "    image = tf.image.decode_png(image, channels=3,dtype=tf.dtypes.uint16)\n",
    "    image = tf.cast(image, tf.float32)/65535\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7fb8a3",
   "metadata": {},
   "source": [
    "## Model Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397cf1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_symmetry(image,lab):\n",
    "    i = tf.random.uniform(shape=(), minval=0, maxval=7, dtype=tf.int32)\n",
    "    if i == 0:\n",
    "        return image,lab\n",
    "    elif i == 1:\n",
    "        return tf.image.transpose(image),lab\n",
    "    elif i == 2:\n",
    "        return tf.image.rot90(image, k=1, name=None),lab\n",
    "    elif i == 3:\n",
    "        return tf.image.rot90(image, k=2, name=None),lab\n",
    "    elif i ==4:\n",
    "        return tf.image.rot90(image, k=3, name=None),lab\n",
    "    elif i == 5:\n",
    "        return tf.image.flip_left_right(image),lab\n",
    "    elif i == 6:\n",
    "        return tf.image.flip_up_down(image),lab\n",
    "    else:\n",
    "        return image,lab\n",
    "def parse_image(joint):\n",
    "    join1 = tf.experimental.numpy.append(load_img(joint[1]),load_img(joint[2]),axis=2)\n",
    "    combo = tf.experimental.numpy.append(join1,load_img(joint[3]),axis=2)\n",
    "    value = int(joint[0])/100\n",
    "    return combo,value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942ebc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_joint,val_joint,test_joint=txt_to_list(Train_txt),txt_to_list(val_txt),txt_to_list(Test_txt)\n",
    "BUFFER_SIZE,BATCH_SIZE = 20000,500\n",
    "'''See if you can get batch size of 500'''\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_joint)\n",
    "train_dataset = train_dataset.map(parse_image)\n",
    "train_dataset=train_dataset.map(apply_symmetry)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=BUFFER_SIZE, seed=SEED)\n",
    "train_dataset = train_dataset.repeat().batch(BATCH_SIZE)\n",
    "#Creating Validation dataset\n",
    "val_dataset=tf.data.Dataset.from_tensor_slices(val_joint)\n",
    "val_dataset = val_dataset.map(parse_image).batch(len(val_joint))\n",
    "#Creating Test dataset\n",
    "test_dataset=tf.data.Dataset.from_tensor_slices(test_joint)\n",
    "test_dataset = test_dataset.map(parse_image).batch(len(test_joint))\n",
    "\n",
    "print(train_dataset)\n",
    "print(val_dataset)\n",
    "print(test_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1ff5ce",
   "metadata": {},
   "source": [
    "## Model Tuning\n",
    "#### This section is for finding the optimal Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73616878",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_name='CNN3size23'\n",
    "def CNN_model(lr,u,u2,u3,u4): # the last tested. good for more data.\n",
    "    model=Sequential()\n",
    "    model.add(Conv2D(u, (3, 3), activation='relu', padding='same',input_shape=(3,3,9)))\n",
    "    model.add(Conv2D(u, (3, 3), activation='relu', padding='valid'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(u2, activation='relu'))\n",
    "    model.add(Dense(u2, activation='relu'))\n",
    "    model.add(Dense(u3, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss= \"MeanAbsoluteError\",\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "                  metrics=[tf.keras.metrics.MeanAbsoluteError(),tf.keras.metrics.MeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b278be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_model(hparam,session_n):\n",
    "    '''Defining the parameters to what will get passed into the NN'''\n",
    "    spe,lr,u,u2,u3,u4=hparam[0],hparam[1],hparam[2],hparam[3],hparam[4],hparam[5]\n",
    "    model = CNN_model(lr,u,u2,u3,u4) #defining NN with the parameter\n",
    "    start=time.time()\n",
    "    callbacks = [] #defining callbacks\n",
    "    callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10))\n",
    "    callbacks.append(tf.keras.callbacks.ModelCheckpoint(temp_weights, save_best_only=True, save_weights_only=True))\n",
    "    model_history=model.fit(train_dataset,validation_data=val_dataset,validation_steps=1,callbacks=callbacks,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            steps_per_epoch=spe,\n",
    "                            epochs=500)#running the model\n",
    "    MAE=get_model_mae(model,temp_weights,val_dataset)\n",
    "    end=time.time()\n",
    "    runtime=str(\"%.2f\"%(end-start))\n",
    "    model_name= \"{}-{}mae-{}r-{}p-{}sec\".format(NN_name,str(round(MAE,5)),session_n,str(hparam),runtime)\n",
    "    save_nn_for_log(model,temp_weights,val_dataset,logpath,model_name) #saving the model with informative info\n",
    "    plot_hist_save(model,temp_weights,val_dataset,model_history,logpath,model_name)\n",
    "    print('MAE is {} in {}sec'.format(MAE,runtime))\n",
    "    models_dic[model_name]=MAE\n",
    "    return MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afdd43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_best_scatter (dataset,datasetstr):\n",
    "    lowest = min(models_dic, key=models_dic.get)\n",
    "    hparam=hp_MAE_dic[min(hp_MAE_dic)]\n",
    "    '''Defining the parameters to what will get passed into the NN'''\n",
    "    spe,lr,u,u2,u3,u4=hparam[0],hparam[1],hparam[2],hparam[3],hparam[4],hparam[5]\n",
    "    model = CNN_model(lr,u,u2,u3,u4) #defining NN with the parameter\n",
    "    model_weights_path = r\"D:\\Training_Data_Creation\\01-logs\\logs{}\\{}\".format(NN_name,lowest)\n",
    "    files=os.listdir(model_weights_path)\n",
    "    model.load_weights(model_weights_path+'\\\\'+files[1])\n",
    "    #set result str to the string version of the dataset\n",
    "    print('Best Scatterplot {}'.format(lowest))\n",
    "    make_scatter_from_results(nnDset_to_Results(model,dataset),datasetstr,'01-Best {} scatters'.format(NN_name),logpath)\n",
    "def create_txt_and_prints():\n",
    "    txtfile=logpath+r'/01-{}Logs.txt'.format(NN_name)\n",
    "    best5_models=sorted(models_dic, key=models_dic.get, reverse=False)[:10]\n",
    "    worst5_models=sorted(models_dic, key=models_dic.get, reverse=True)[:10]\n",
    "    with open(txtfile, 'w') as f:\n",
    "        f.write(\"Overall runtime is \"+str(\"%.2f\"%(endmain-startmain))+' sec\\n')\n",
    "        f.write('Order goes SPE, BS, LR, Units (first layer then rest), MAE \\n Best 10 Models \\n')\n",
    "        for line in list(best5_models):\n",
    "            f.write(str(line))\n",
    "            f.write('\\n')\n",
    "        f.write('\\n Worst 10 Models \\n')\n",
    "        for line in list(worst5_models):\n",
    "            f.write(str(line))\n",
    "            f.write('\\n')\n",
    "        f.write('\\n All Models \\n')\n",
    "        for line in list(models_dic):\n",
    "            f.write(str(line))\n",
    "            f.write('\\n')\n",
    "    print('Logs created, Best 10 models are:')\n",
    "    for i in best5_models:\n",
    "        print(i)\n",
    "    print('\\n Worst 10 models are:')\n",
    "    for i in worst5_models:\n",
    "        print(i)\n",
    "    print('\\n')\n",
    "    print(\"Overall runtime is \"+str(\"%.2f\"%(endmain-startmain))+' sec')\n",
    "def display_best_history():\n",
    "    lowest = min(models_dic, key=models_dic.get)\n",
    "    models_dic[lowest]\n",
    "    model_weights_path = r\"D:\\Training_Data_Creation\\01-logs\\logs{}\\{}\".format(NN_name,lowest)\n",
    "    files=os.listdir(model_weights_path)\n",
    "    imgstr=files[0]\n",
    "    img = mpimg.imread(model_weights_path+'\\\\'+imgstr)\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1856bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_n=0\n",
    "startmain = time.time()\n",
    "hp_MAE_dic={}\n",
    "hp_dic={'hp_lr':[.001],'hp_spe':[80,90,70],\n",
    "        'hp_u':[28,24,20],'hp_u2':[28,24,20],'hp_u3':[28,24,20],'hp_u4':[0]}\n",
    "models_dic={}\n",
    "logpath=r\"D:\\Training_Data_Creation\\01-logs\\logs{}\".format(NN_name)\n",
    "for spe in hp_dic['hp_spe']:\n",
    "    for lr in hp_dic['hp_lr']:\n",
    "        for u in hp_dic['hp_u']:\n",
    "            for u2 in hp_dic['hp_u2']:\n",
    "                for u3 in hp_dic['hp_u3']:\n",
    "                    for u4 in hp_dic['hp_u4']:\n",
    "                        hparam=[spe,lr,u,u2,u3,u4]\n",
    "                        print('--- Session {}: Testing {} spe, {} bs, {} lr and {},{},{},{} units'.format(session_n,spe,BATCH_SIZE,lr,u,u2,u3,u4))\n",
    "                        MAE=assess_model(hparam,session_n)\n",
    "                        plt.close()\n",
    "                        hp_MAE_dic[MAE]=hparam\n",
    "                        session_n+=1\n",
    "endmain = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0b2c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Functions work on most recently assessed model'''\n",
    "endmain = time.time()\n",
    "create_txt_and_prints()\n",
    "create_best_scatter (val_dataset,'Val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de8a969",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_best_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7702cb9e",
   "metadata": {},
   "source": [
    "## Assess Different Models\n",
    "#### For evaluating test dataset and other datasets after model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3b8c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#swap for testdset\n",
    "create_best_scatter (test_dataset,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac8fb6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
