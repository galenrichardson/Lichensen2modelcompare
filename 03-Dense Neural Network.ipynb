{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5546d53c",
   "metadata": {},
   "source": [
    "Citation for code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a5f3fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os, shutil, time, csv,math,scipy,matplotlib\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr,gaussian_kde\n",
    "#displaying data\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow,figure\n",
    "import matplotlib.image as mpimg\n",
    "plt.rcParams['figure.dpi'] = 300 \n",
    "%matplotlib inline\n",
    "#TF imports\n",
    "import tensorflow as tf\n",
    "'''TF code to tell TF version, GPU detected, and limit memory growth'''\n",
    "print(f\"Tensorflow ver. {tf.__version__}\")\n",
    "physical_device = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(f'Device found : {physical_device}')\n",
    "tf.config.experimental.get_memory_growth(physical_device[0])\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential,Model,load_model\n",
    "from tensorflow.keras.layers import Input, Conv2D,Conv1D, Dense, Flatten, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a26dfb0",
   "metadata": {},
   "source": [
    "### Variables and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534d0a5c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Train_csv = r'D:\\Training_Data_Creation\\Pointer_files\\Train_Tiles40.csv'\n",
    "Val_csv = r'D:\\Training_Data_Creation\\Pointer_files\\Val_Tiles40.csv'\n",
    "Test_csv = r'D:\\Training_Data_Creation\\Pointer_files\\Test_Tiles40.csv'\n",
    "#temp weights which get overwriten by save_nn_for_log\n",
    "temp_weights=r'D:\\Training_Data_Creation\\01-logs\\temp.h5'\n",
    "SEED = 71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275a239f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def process_csv_to_datasets(incsv):\n",
    "    dset=pd.read_csv(incsv)\n",
    "    dset_labels = dset.pop('Trueval')\n",
    "    dset= np.array(dset)/65535 #divide by 65535 since that is the max 16bit int value\n",
    "    dset= tf.dtypes.cast(dset,'float32')\n",
    "     #divide by 100 since that is max mask pixel value\n",
    "    dset_labels=np.array(dset_labels)/100\n",
    "    dset_labels=tf.dtypes.cast(dset_labels,'float32')\n",
    "    return dset,dset_labels\n",
    "#splitting the datasets into values and labels\n",
    "train,train_labels=process_csv_to_datasets(Train_csv)\n",
    "val,val_labels=process_csv_to_datasets(Val_csv)\n",
    "test,test_labels=process_csv_to_datasets(Test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd2465a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import ipynb.fs \n",
    "#Importing metric functions\n",
    "from .defs.Thesis_Functions import dtime,calculate_metrics,time_and_metrics\n",
    "#Importing plotting functions\n",
    "from .defs.Thesis_Functions import make_scatter_from_results,plot_hist_save,Feature_importance\n",
    "#Importing NN \n",
    "from .defs.Thesis_Functions import get_model_mae,save_nn_for_log,nnDset_to_Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b6cfcf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''Creating Datasets for Dense Neaural Network'''\n",
    "BUFFER_SIZE,BATCH_SIZE=200000,1000\n",
    "#training it in batches to maximize speed\n",
    "train_dataset=tf.data.Dataset.from_tensor_slices((train,train_labels))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=BUFFER_SIZE, seed=SEED).repeat().batch(BATCH_SIZE)#batchsize=batchsize for training\n",
    "val_dataset=tf.data.Dataset.from_tensor_slices((val,val_labels)).batch(len(val))#batchsize == number of individual values\n",
    "test_dataset=tf.data.Dataset.from_tensor_slices((test,test_labels)).batch(len(test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839db937",
   "metadata": {},
   "source": [
    "## Model Tuning\n",
    "#### This section is for finding the optimal Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430c6779",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NN_name='NN4'\n",
    "def Dense_NN(lr,u,u2,u3,u4):\n",
    "    model=Sequential()\n",
    "    model.add(Dense(u, activation='relu', input_shape=[9]))\n",
    "    model.add(Dense(u2, activation='relu'))\n",
    "    model.add(Dense(u3, activation='relu'))\n",
    "    model.add(Dense(u4, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss= \"MeanAbsoluteError\",\n",
    "                  optimizer=keras.optimizers.adam(learning_rate=lr),\n",
    "                  metrics=[tf.keras.metrics.MeanAbsoluteError(),tf.keras.metrics.MeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce48ea1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_best_scatter (dataset,datasetstr):\n",
    "    lowest = min(models_dic, key=models_dic.get)\n",
    "    hparam=hp_MAE_dic[min(hp_MAE_dic)]\n",
    "    '''Defining the parameters to what will get passed into the NN'''\n",
    "    spe,lr,u,u2,u3,u4=hparam[0],hparam[1],hparam[2],hparam[3],hparam[4],hparam[5]\n",
    "    model = Dense_NN(lr,u,u2,u3,u4) #defining NN with the parameter\n",
    "    model_weights_path = r\"D:\\Training_Data_Creation\\01-logs\\logs{}\\{}\".format(NN_name,lowest)\n",
    "    files=os.listdir(model_weights_path)\n",
    "    model.load_weights(model_weights_path+'\\\\'+files[1])\n",
    "    #set result str to the string version of the dataset\n",
    "    print('Best Scatterplot {}'.format(lowest))\n",
    "    make_scatter_from_results(nnDset_to_Results(model,dataset),datasetstr,'01-Best {} scatters'.format(NN_name),logpath)\n",
    "\n",
    "def create_txt_and_prints():\n",
    "    txtfile=logpath+r'/01-{}Logs.txt'.format(NN_name)\n",
    "    best5_models=sorted(models_dic, key=models_dic.get, reverse=False)[:10]\n",
    "    worst5_models=sorted(models_dic, key=models_dic.get, reverse=True)[:10]\n",
    "    with open(txtfile, 'w') as f:\n",
    "        f.write(\"Overall runtime is \"+str(\"%.2f\"%(endmain-startmain))+' sec\\n')\n",
    "        f.write('Order goes SPE, BS, LR, Units (first layer then rest), MAE \\n Best 10 Models \\n')\n",
    "        for line in list(best5_models):\n",
    "            f.write(str(line))\n",
    "            f.write('\\n')\n",
    "        f.write('\\n Worst 10 Models \\n')\n",
    "        for line in list(worst5_models):\n",
    "            f.write(str(line))\n",
    "            f.write('\\n')\n",
    "        f.write('\\n All Models \\n')\n",
    "        for line in list(models_dic):\n",
    "            f.write(str(line))\n",
    "            f.write('\\n')\n",
    "    print('Logs created, Best 10 models are:')\n",
    "    for i in best5_models:\n",
    "        print(i)\n",
    "    print('\\n Worst 10 models are:')\n",
    "    for i in worst5_models:\n",
    "        print(i)\n",
    "    print('\\n')\n",
    "    print(\"Overall runtime is \"+str(\"%.2f\"%(endmain-startmain))+' sec')\n",
    "def display_best_history():\n",
    "    lowest = min(models_dic, key=models_dic.get)\n",
    "    models_dic[lowest]\n",
    "    model_weights_path = r\"D:\\Training_Data_Creation\\01-logs\\logs{}\\{}\".format(NN_name,lowest)\n",
    "    files=os.listdir(model_weights_path)\n",
    "    imgstr=files[0]\n",
    "    img = mpimg.imread(model_weights_path+'\\\\'+imgstr)\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec6f558",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def assess_model(hparam,session_n):\n",
    "    '''Defining the parameters to what will get passed into the NN'''\n",
    "    spe,lr,u,u2,u3,u4=hparam[0],hparam[1],hparam[2],hparam[3],hparam[4],hparam[5]\n",
    "    model = Dense_NN(lr,u,u2,u3,u4) #defining NN with the parameter\n",
    "    start=time.time()\n",
    "    callbacks = [] #defining callbacks\n",
    "    callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10))\n",
    "    callbacks.append(tf.keras.callbacks.ModelCheckpoint(temp_weights, save_best_only=True, save_weights_only=True))\n",
    "    model_history=model.fit(train_dataset,validation_data=val_dataset,validation_steps=1,callbacks=callbacks,\n",
    "                            steps_per_epoch=spe,\n",
    "                            epochs=500)#running the model\n",
    "    MAE=get_model_mae(model,temp_weights,val_dataset)\n",
    "    end=time.time()\n",
    "    runtime=str(\"%.2f\"%(end-start))\n",
    "    model_name= \"{}-{}mae-{}r-{}p-{}sec\".format(NN_name,str(round(MAE,5)),session_n,str(hparam),runtime)\n",
    "    save_nn_for_log(model,temp_weights,val_dataset,logpath,model_name) #saving the model with informative info\n",
    "    plot_hist_save(model,temp_weights,val_dataset,model_history,logpath,model_name)\n",
    "    print('MAE is {} in {}sec'.format(MAE,runtime))\n",
    "    models_dic[model_name]=MAE\n",
    "    return MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0fd970",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "session_n=0\n",
    "startmain = time.time()\n",
    "hp_MAE_dic={}\n",
    "hp_dic={'hp_lr':[.001],'hp_spe':[100],\n",
    "        'hp_u':[32,24,16,8],'hp_u2':[32,24,16,8],'hp_u3':[32,24,16,8],'hp_u4':[32,24,16,8]}\n",
    "models_dic={}\n",
    "logpath=r\"D:\\Training_Data_Creation\\01-logs\\logs{}\".format(NN_name)\n",
    "for spe in hp_dic['hp_spe']:\n",
    "    for lr in hp_dic['hp_lr']:\n",
    "        for u in hp_dic['hp_u']:\n",
    "            for u2 in hp_dic['hp_u2']:\n",
    "                for u3 in hp_dic['hp_u3']:\n",
    "                    for u4 in hp_dic['hp_u4']:\n",
    "                        hparam=[spe,lr,u,u2,u3,u4]\n",
    "                        print('--- Session {}: Testing {} spe, {} bs, {} lr and {},{},{},{} units'.format(session_n,spe,BATCH_SIZE,lr,u,u2,u3,u4))\n",
    "                        MAE=assess_model(hparam,session_n)\n",
    "                        plt.close()\n",
    "                        hp_MAE_dic[MAE]=hparam\n",
    "                        session_n+=1\n",
    "endmain = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeed98c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Functions work on most recently assessed model'''\n",
    "create_txt_and_prints()\n",
    "create_best_scatter (val_dataset,'Val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e950080e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display_best_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad287ac9",
   "metadata": {},
   "source": [
    "## Model Tuning with LR Scheduler\n",
    "#### This section is for finding the optimal Hyperparameters with LR Schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1c9bd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr=.001\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch <= 20:\n",
    "        return lr\n",
    "    else:\n",
    "        punish_val=int(epoch/20)\n",
    "        return lr/((punish_val*.1+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7568a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr=.001\n",
    "x,y=[],[]\n",
    "for i in range(0,100):\n",
    "    x.append(i)\n",
    "    y.append(scheduler(i, lr))\n",
    "plt.plot(x,y, color='black')\n",
    "plt.xlabel('Epoch',size=14)\n",
    "plt.ylabel('Learning Rate',size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811723b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def assess_model_schedule(hparam,session_n):\n",
    "    '''Defining the parameters to what will get passed into the NN'''\n",
    "    spe,u,u2,u3,u4=hparam[0],hparam[1],hparam[2],hparam[3],hparam[4]\n",
    "    model = Dense_NN(u,u2,u3,u4) #defining NN with the parameter\n",
    "    start=time.time()\n",
    "    callbacks = [] #defining callbacks\n",
    "    callbacks.append(tf.keras.callbacks.LearningRateScheduler(scheduler))\n",
    "    callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15))\n",
    "    callbacks.append(tf.keras.callbacks.ModelCheckpoint(temp_weights, save_best_only=True, save_weights_only=True))\n",
    "    model_history=model.fit(train_dataset,validation_data=val_dataset,validation_steps=1,callbacks=callbacks,\n",
    "                            steps_per_epoch=spe,\n",
    "                            epochs=500)#running the model\n",
    "    MAE=get_model_mae(model,temp_weights,val_dataset)\n",
    "    end=time.time()\n",
    "    runtime=str(\"%.2f\"%(end-start))\n",
    "    model_name= \"{}-{}mae-{}r-{}p-{}sec\".format(NN_name,str(round(MAE,5)),session_n,str(hparam),runtime)\n",
    "    save_nn_for_log(model,temp_weights,val_dataset,logpath,model_name) #saving the model with informative info\n",
    "    plot_hist_save(model,temp_weights,val_dataset,model_history,logpath,model_name)\n",
    "    print('MAE is {} in {}sec'.format(MAE,runtime))\n",
    "    models_dic[model_name]=MAE\n",
    "    return MAE\n",
    "def create_best_scatter (dataset,datasetstr):\n",
    "    lowest = min(models_dic, key=models_dic.get)\n",
    "    hparam=hp_MAE_dic[min(hp_MAE_dic)]\n",
    "    '''Defining the parameters to what will get passed into the NN'''\n",
    "    spe,u,u2,u3,u4=hparam[0],hparam[1],hparam[2],hparam[3],hparam[4]\n",
    "    model = Dense_NN(u,u2,u3,u4) #defining NN with the parameter\n",
    "    model_weights_path = r\"D:\\Training_Data_Creation\\01-logs\\logs{}\\{}\".format(NN_name,lowest)\n",
    "    files=os.listdir(model_weights_path)\n",
    "    model.load_weights(model_weights_path+'\\\\'+files[1])\n",
    "    #set result str to the string version of the dataset\n",
    "    print('Best Scatterplot {}'.format(lowest))\n",
    "    make_scatter_from_results(nnDset_to_Results(model,dataset),datasetstr,'01-Best {} scatters'.format(NN_name),logpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383b17fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NN_name='NN4_1000bs_relu_LRscheduler'\n",
    "def Dense_NN(u,u2,u3,u4): # the last tested. good for more data.\n",
    "    model=Sequential()\n",
    "    model.add(Dense(u, activation='relu', input_shape=[9]))\n",
    "    model.add(Dense(u2, activation='relu'))\n",
    "    model.add(Dense(u3, activation='relu'))\n",
    "    model.add(Dense(u4, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss= \"MeanAbsoluteError\",\n",
    "                  optimizer=keras.optimizers.Adam(),\n",
    "                  metrics=[tf.keras.metrics.MeanAbsoluteError(),tf.keras.metrics.MeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcbadd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_n=0\n",
    "startmain = time.time()\n",
    "hp_MAE_dic={}\n",
    "hp_dic={'hp_spe':[150,200],\n",
    "        'hp_u':[40,32,24,16],'hp_u2':[40,32,24,16],'hp_u3':[40,32,24,16],'hp_u4':[40,32,24,16]}\n",
    "models_dic={}\n",
    "logpath=r\"D:\\Training_Data_Creation\\01-logs\\logs{}\".format(NN_name)\n",
    "for spe in hp_dic['hp_spe']:\n",
    "    for u in hp_dic['hp_u']:\n",
    "        for u2 in hp_dic['hp_u2']:\n",
    "            for u3 in hp_dic['hp_u3']:\n",
    "                for u4 in hp_dic['hp_u4']:\n",
    "                    hparam=[spe,u,u2,u3,u4]\n",
    "                    print('--- Session {}: Testing {} spe, {} bs, and {},{},{},{} units'.format(session_n,spe,BATCH_SIZE,u,u2,u3,u4))\n",
    "                    MAE=assess_model_schedule(hparam,session_n)\n",
    "                    plt.close()\n",
    "                    hp_MAE_dic[MAE]=hparam\n",
    "                    session_n+=1\n",
    "endmain = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4f8536",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Functions work on most recently assessed model'''\n",
    "create_txt_and_prints()\n",
    "create_best_scatter (val_dataset,'Val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c07014",
   "metadata": {},
   "source": [
    "## Assess Different Models\n",
    "#### For evaluating test dataset and other datasets after model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1065678b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NN_name='NN4'\n",
    "def Dense_NN(lr,u,u2,u3,u4): # the last tested. good for more data.\n",
    "    model=Sequential()\n",
    "    model.add(Dense(u, activation='relu', input_shape=[9]))\n",
    "    model.add(Dense(u2, activation='relu'))\n",
    "    model.add(Dense(u3, activation='relu'))\n",
    "    model.add(Dense(u4, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss= \"MeanAbsoluteError\",\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "                  metrics=[tf.keras.metrics.MeanAbsoluteError(),tf.keras.metrics.MeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471d2638",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr,u,u2,u3,u4= 0.001, 16, 16, 32,40\n",
    "path_to_weights=r'D:\\Training_Data_Creation\\01-logs\\Best_dnn\\01SELECTEDNN4_1000bs_2-0.05139mae-1012r-[225, 0.001, 16, 16, 32, 40]p-66.64sec\\Weights-0.05139_11182022_070752.h5'\n",
    "def load_model_and_assess(nndset,dsetstr):\n",
    "    model=Dense_NN(lr,u,u2,u3,u4)\n",
    "    model.load_weights(path_to_weights)\n",
    "    #set result str to the string version of the dataset\n",
    "    model.summary()\n",
    "    make_scatter_from_results(nnDset_to_Results(model,nndset),dsetstr,'01-Best {} scatters'.format(NN_name),r'D:\\Training_Data_Creation\\Results_Scatter\\Dense_NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288b1178",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "load_model_and_assess(val_dataset,'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d0bc12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
